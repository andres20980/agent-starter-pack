{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licenciado bajo la Licencia Apache, Versión 2.0 (la \"Licencia\");\n",
        "# no puedes usar este archivo excepto en cumplimiento con la Licencia.\n",
        "# Puedes obtener una copia de la Licencia en\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# A menos que lo requiera la ley aplicable o se acuerde por escrito, el software\n",
        "# distribuido bajo la Licencia se distribuye \"TAL CUAL\",\n",
        "# SIN GARANTÍAS NI CONDICIONES DE NINGÚN TIPO, ya sean expresas o implícitas.\n",
        "# Consulta la Licencia para conocer el lenguaje específico que rige los permisos y\n",
        "# limitaciones bajo la Licencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Evaluación de Agentes - Evaluar un agente LangGraph con Vertex AI Gen AI Evaluation\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Logo de Google Colaboratory\"><br> Abrir en Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fevaluation%2Fevaluating_langgraph_agent.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Logo de Google Cloud Colab Enterprise\"><br> Abrir en Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Logo de Vertex AI\"><br> Abrir en Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"Logo de GitHub\"><br> Ver en GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Compartir en:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Logo de LinkedIn\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Logo de Bluesky\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"Logo de X\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Logo de Reddit\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluating_langgraph_agent.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Logo de Facebook\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Autores | [Ivan Nardini](https://github.com/inardini) [Naveksha Sood](https://github.com/navekshasood)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Resumen\n",
        "\n",
        "Al igual que cualquier aplicación de IA generativa, los agentes de IA requieren una evaluación exhaustiva para garantizar que funcionen de manera confiable y efectiva. Esta evaluación debe realizarse tanto en tiempo real (en línea) como en grandes conjuntos de datos de casos de prueba (fuera de línea). Los desarrolladores que crean aplicaciones de agentes enfrentan un desafío significativo al evaluar su rendimiento. Tanto las evaluaciones subjetivas (retroalimentación humana) como las objetivas (métricas medibles) son esenciales para generar confianza en el comportamiento del agente.\n",
        "\n",
        "Vertex AI Model Evaluation proporciona un conjunto de herramientas de métodos y métricas controladas por calidad y explicables para evaluar cualquier modelo o aplicación generativa, incluidos los agentes, y comparar los resultados de la evaluación con tu propio juicio, utilizando tus propios criterios de evaluación.\n",
        "\n",
        "Este tutorial muestra cómo evaluar un agente LangGraph utilizando Vertex AI Gen AI Evaluation para la evaluación de agentes.\n",
        "\n",
        "El tutorial utiliza los siguientes servicios y recursos de Google Cloud:\n",
        "\n",
        "*  Vertex AI Gen AI Evaluation\n",
        "\n",
        "Los pasos realizados incluyen:\n",
        "\n",
        "* Construir un agente local utilizando LangGraph\n",
        "* Preparar el conjunto de datos de evaluación del agente\n",
        "* Evaluación del uso de una única herramienta\n",
        "* Evaluación de trayectoria\n",
        "* Evaluación de respuesta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Empezar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Instalar Vertex AI SDK y otros paquetes requeridos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet \"google-cloud-aiplatform[evaluation]\" \\\n",
        "    \"langchain_google_vertexai\" \\\n",
        "    \"langgraph\" \\\n",
        "    \"cloudpickle==3.0.0\" \\\n",
        "    \"pydantic==2.7.4\" \\\n",
        "    \"requests\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Reiniciar el entorno de ejecución\n",
        "\n",
        "Para usar los paquetes recién instalados en este entorno de Jupyter, debes reiniciar el entorno de ejecución. Puedes hacerlo ejecutando la celda a continuación, que reinicia el kernel actual.\n",
        "\n",
        "El reinicio puede tardar un minuto o más. Después de que se haya reiniciado, continúa con el siguiente paso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ El kernel se va a reiniciar. En Colab o Colab Enterprise, es posible que veas un mensaje de error que dice \"Tu sesión se ha bloqueado por una razón desconocida.\" Esto es esperado. Espera hasta que haya terminado antes de continuar con el siguiente paso. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Autenticar tu entorno de notebook (solo Colab)\n",
        "\n",
        "Si estás ejecutando este notebook en Google Colab, ejecuta la celda a continuación para autenticar tu entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Configurar la información del proyecto de Google Cloud e inicializar Vertex AI SDK\n",
        "\n",
        "Para comenzar a usar Vertex AI, debes tener un proyecto de Google Cloud existente y [habilitar la API de Vertex AI](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Obtén más información sobre [cómo configurar un proyecto y un entorno de desarrollo](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Usa la variable de entorno si el usuario no proporciona el ID del proyecto.\n",
        "import os\n",
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "EXPERIMENT_NAME = \"evaluate-langgraph-agent\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "## Importar bibliotecas\n",
        "\n",
        "Importar bibliotecas del tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "import random\n",
        "import string\n",
        "from typing import Literal\n",
        "\n",
        "from IPython.display import HTML, Markdown, display\n",
        "\n",
        "# Evaluar agente\n",
        "from google.cloud import aiplatform\n",
        "from langchain.load import dump as langchain_load_dump\n",
        "\n",
        "# Construir agente\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langgraph.graph import END, MessageGraph\n",
        "from langgraph.prebuilt import ToolNode\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from vertexai.preview.evaluation import EvalTask\n",
        "from vertexai.preview.evaluation.metrics import (\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        "    TrajectorySingleToolUse,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVnBDX54gz7j"
      },
      "source": [
        "## Definir funciones auxiliares\n",
        "\n",
        "Iniciar un conjunto de funciones auxiliares para imprimir los resultados del tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSgWjMD_g1_v"
      },
      "outputs": [],
      "source": [
        "def get_id(length: int = 8) -> str:\n",
        "    \"\"\"Generar un uuid de una longitud especificada (por defecto=8).\"\"\"\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "def parse_messages_to_output_dictionary(messages: list[dict]) -> dict:\n",
        "    \"\"\"Analizar la respuesta y las llamadas a funciones de una lista de mensajes en el formato del constructor.\"\"\"\n",
        "\n",
        "    final_output = {\n",
        "        \"response\": \"No se encontró respuesta de IA en el historial de mensajes.\",\n",
        "        \"predicted_trajectory\": [],\n",
        "    }\n",
        "\n",
        "    # Procesar cada mensaje\n",
        "    function_calls = []\n",
        "    for message in messages:\n",
        "        # Verificar si es un mensaje de herramienta que contiene la respuesta real\n",
        "        if message.get(\"type\") == \"constructor\" and \"ToolMessage\" in message.get(\n",
        "            \"id\", []\n",
        "        ):\n",
        "            final_output[\"response\"] = message[\"kwargs\"][\"content\"]\n",
        "\n",
        "        # Verificar si es un mensaje de IA para obtener llamadas a herramientas\n",
        "        elif message.get(\"type\") == \"constructor\" and \"AIMessage\" in message.get(\n",
        "            \"id\", []\n",
        "        ):\n",
        "            tool_calls = message[\"kwargs\"].get(\"tool_calls\", [])\n",
        "            for tool_call in tool_calls:\n",
        "                if tool_call:\n",
        "                    function_calls.append(\n",
        "                        {\n",
        "                            \"tool_name\": tool_call.get(\"name\"),\n",
        "                            \"tool_input\": tool_call.get(\"args\"),\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "    final_output[\"predicted_trajectory\"] = function_calls\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def format_output_as_markdown(output: dict) -> str:\n",
        "    \"\"\"Convertir el diccionario de salida a una cadena de markdown formateada.\"\"\"\n",
        "    markdown = \"### Respuesta de IA\\n\"\n",
        "    markdown += f\"{output['response']}\\n\\n\"\n",
        "\n",
        "    if output[\"predicted_trajectory\"]:\n",
        "        markdown += \"### Llamadas a Funciones\\n\"\n",
        "        for call in output[\"predicted_trajectory\"]:\n",
        "            markdown += f\"- **Función**: `{call['tool_name']}`\\n\"\n",
        "            markdown += \"  - **Argumentos**:\\n\"\n",
        "            for key, value in call[\"tool_input\"].items():\n",
        "                markdown += f\"    - `{key}`: `{value}`\\n\"\n",
        "\n",
        "    return markdown\n",
        "\n",
        "\n",
        "def display_eval_report(eval_result: pd.DataFrame) -> None:\n",
        "    \"\"\"Mostrar los resultados de la evaluación.\"\"\"\n",
        "    metrics_df = pd.DataFrame.from_dict(eval_result.summary_metrics, orient=\"index\").T\n",
        "    display(Markdown(\"### Métricas Resumidas\"))\n",
        "    display(metrics_df)\n",
        "\n",
        "    display(Markdown(f\"### Métricas por Fila\"))\n",
        "    display(eval_result.metrics_table)\n",
        "\n",
        "\n",
        "def display_drilldown(row: pd.Series) -> None:\n",
        "    \"\"\"Mostrar una vista detallada para los datos de trayectoria dentro de una fila.\"\"\"\n",
        "\n",
        "    style = \"white-space: pre-wrap; width: 800px; overflow-x: auto;\"\n",
        "\n",
        "    if not (\n",
        "        isinstance(row[\"predicted_trajectory\"], list)\n",
        "        and isinstance(row[\"reference_trajectory\"], list)\n",
        "    ):\n",
        "        return\n",
        "\n",
        "    for predicted_trajectory, reference_trajectory in zip(\n",
        "        row[\"predicted_trajectory\"], row[\"reference_trajectory\"]\n",
        "    ):\n",
        "        display(\n",
        "            HTML(\n",
        "                f\"<h3>Nombres de Herramientas:</h3><div style='{style}'>{predicted_trajectory['tool_name'], reference_trajectory['tool_name']}</div>\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if not (\n",
        "            isinstance(predicted_trajectory.get(\"tool_input\"), dict)\n",
        "            and isinstance(reference_trajectory.get(\"tool_input\"), dict)\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        for tool_input_key in predicted_trajectory[\"tool_input\"]:\n",
        "            print(\"Clave de Entrada de Herramienta: \", tool_input_key)\n",
        "\n",
        "            if tool_input_key in reference_trajectory[\"tool_input\"]:\n",
        "                print(\n",
        "                    \"Valores de Herramienta: \",\n",
        "                    predicted_trajectory[\"tool_input\"][tool_input_key],\n",
        "                    reference_trajectory[\"tool_input\"][tool_input_key],\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    \"Valores de Herramienta: \",\n",
        "                    predicted_trajectory[\"tool_input\"][tool_input_key],\n",
        "                    \"N/A\",\n",
        "                )\n",
        "        print(\"\\n\")\n",
        "    display(HTML(\"<hr>\"))\n",
        "\n",
        "\n",
        "def display_dataframe_rows(\n",
        "    df: pd.DataFrame,\n",
        "    columns: list[str] | None = None,\n",
        "    num_rows: int = 3,\n",
        "    display_drilldown: bool = False,\n",
        ") -> None:\n",
        "    \"\"\"Mostrar un subconjunto de filas de un DataFrame, opcionalmente incluyendo una vista detallada.\"\"\"\n",
        "\n",
        "    if columns:\n",
        "        df = df[columns]\n",
        "\n",
        "    base_style = \"font-family: monospace; font-size: 14px; white-space: pre-wrap; width: auto; overflow-x: auto;\"\n",
        "    header_style = base_style + \"font-weight: bold;\"\n",
        "\n",
        "    for _, row in df.head(num_rows).iterrows():\n",
        "        for column in df.columns:\n",
        "            display(\n",
        "                HTML(\n",
        "                    f\"<span style='{header_style}'>{column.replace('_', ' ').title()}: </span>\"\n",
        "                )\n",
        "            )\n",
        "            display(HTML(f\"<span style='{base_style}'>{row[column]}</span><br>\"))\n",
        "\n",
        "        display(HTML(\"<hr>\"))\n",
        "\n",
        "        if (\n",
        "            display_drilldown\n",
        "            and \"predicted_trajectory\" in df.columns\n",
        "            and \"reference_trajectory\" in df.columns\n",
        "        ):\n",
        "            display_drilldown(row)\n",
        "\n",
        "\n",
        "def plot_bar_plot(\n",
        "    eval_result: pd.DataFrame, title: str, metrics: list[str] = None\n",
        ") -> None:\n",
        "    fig = go.Figure()\n",
        "    data = []\n",
        "\n",
        "    summary_metrics = eval_result.summary_metrics\n",
        "    if metrics:\n",
        "        summary_metrics = {\n",
        "            k: summary_metrics[k]\n",
        "            for k, v in summary_metrics.items()\n",
        "            if any(selected_metric in k for selected_metric in metrics)\n",
        "        }\n",
        "\n",
        "    data.append(\n",
        "        go.Bar(\n",
        "            x=list(summary_metrics.keys()),\n",
        "            y=list(summary_metrics.values()),\n",
        "            name=title,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig = go.Figure(data=data)\n",
        "\n",
        "    # Cambiar el modo de la barra\n",
        "    fig.update_layout(barmode=\"group\")\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def display_radar_plot(eval_results, title: str, metrics=None):\n",
        "    \"\"\"Graficar el gráfico de radar.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    summary_metrics = eval_results.summary_metrics\n",
        "    if metrics:\n",
        "        summary_metrics = {\n",
        "            k: summary_metrics[k]\n",
        "            for k, v in summary_metrics.items()\n",
        "            if any(selected_metric in k for selected_metric in metrics)\n",
        "        }\n",
        "\n",
        "    min_val = min(summary_metrics.values())\n",
        "    max_val = max(summary_metrics.values())\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=list(summary_metrics.values()),\n",
        "            theta=list(summary_metrics.keys()),\n",
        "            fill=\"toself\",\n",
        "            name=title,\n",
        "        )\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        polar=dict(radialaxis=dict(visible=True, range=[min_val, max_val])),\n",
        "        showlegend=True,\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDaa2Mtsifmq"
      },
      "source": [
        "## Construir agente LangGraph\n",
        "\n",
        "Construye tu aplicación utilizando LangGraph, incluyendo el modelo Gemini, herramientas personalizadas que defines y un enrutador para controlar el flujo conversacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHwShhpOitKp"
      },
      "source": [
        "### Configurar herramientas\n",
        "\n",
        "Para empezar, configura las herramientas que un agente de soporte al cliente necesita para hacer su trabajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA2ZKvfeislw"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def get_product_details(product_name: str):\n",
        "    \"\"\"Recopila detalles básicos sobre un producto.\"\"\"\n",
        "    details = {\n",
        "        \"smartphone\": \"Un smartphone de última generación con funciones avanzadas de cámara y procesamiento ultrarrápido.\",\n",
        "        \"usb charger\": \"Un cargador usb súper rápido y ligero\",\n",
        "        \"shoes\": \"Zapatillas de running de alto rendimiento diseñadas para comodidad, soporte y velocidad.\",\n",
        "        \"headphones\": \"Auriculares inalámbricos con tecnología avanzada de cancelación de ruido para un audio inmersivo.\",\n",
        "        \"speaker\": \"Un altavoz inteligente controlado por voz que reproduce música, establece alarmas y controla dispositivos domésticos inteligentes.\",\n",
        "    }\n",
        "    return details.get(product_name, \"Detalles del producto no encontrados.\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_product_price(product_name: str):\n",
        "    \"\"\"Recopila el precio de un producto.\"\"\"\n",
        "    details = {\n",
        "        \"smartphone\": 500,\n",
        "        \"usb charger\": 10,\n",
        "        \"shoes\": 100,\n",
        "        \"headphones\": 50,\n",
        "        \"speaker\": 80,\n",
        "    }\n",
        "    return details.get(product_name, \"Precio del producto no encontrado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be70714d9fae"
      },
      "source": [
        "### Definir enrutador\n",
        "\n",
        "Configura un enrutador para dirigir el flujo de la conversación seleccionando la herramienta adecuada según la entrada del usuario o el estado de la interacción.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "516b5108d327"
      },
      "outputs": [],
      "source": [
        "def router(\n",
        "    state: list[BaseMessage],\n",
        ") -> Literal[\"get_product_details\", \"get_product_price\", \"__end__\"]:\n",
        "    \"\"\"Inicia la recuperación de detalles o precios del producto si el usuario lo solicita.\"\"\"\n",
        "    # Obtener las tool_calls del último mensaje en el historial de la conversación.\n",
        "    tool_calls = state[-1].tool_calls\n",
        "\n",
        "    # Si hay alguna tool_call\n",
        "    if tool_calls:\n",
        "        # Verificar el nombre de la función en la primera tool call\n",
        "        function_name = tool_calls[0].get(\"name\")\n",
        "        if function_name == \"get_product_price\":\n",
        "            return \"get_product_price\"\n",
        "        else:\n",
        "            return \"get_product_details\"\n",
        "    else:\n",
        "        # Terminar el flujo de la conversación.\n",
        "        return \"__end__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4mk5XPui4Y1"
      },
      "source": [
        "### Configurar el modelo\n",
        "\n",
        "Elige qué modelo de IA Gemini usará tu agente. Si tienes curiosidad sobre Gemini y sus diferentes capacidades, consulta [la documentación oficial](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) para obtener más detalles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaYeo6K2i-w1"
      },
      "outputs": [],
      "source": [
        "llm = \"gemini-1.5-pro\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNlAY9cojEWz"
      },
      "source": [
        "### Montar el agente\n",
        "\n",
        "El Vertex AI Gen AI Evaluation funciona directamente con agentes 'Queryable', y también te permite agregar tus propias funciones personalizadas con una estructura específica (firma).\n",
        "\n",
        "En este caso, montas el agente utilizando una función personalizada. La función activa el agente para una entrada dada y analiza el resultado del agente para extraer la respuesta y las herramientas llamadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAFdi7SujGP8"
      },
      "outputs": [],
      "source": [
        "def agent_parsed_outcome(input):\n",
        "\n",
        "    model = ChatVertexAI(model=llm)\n",
        "    builder = MessageGraph()\n",
        "\n",
        "    model_with_tools = model.bind_tools([get_product_details, get_product_price])\n",
        "    builder.add_node(\"tools\", model_with_tools)\n",
        "\n",
        "    tool_node = ToolNode([get_product_details, get_product_price])\n",
        "    builder.add_node(\"get_product_details\", tool_node)\n",
        "    builder.add_node(\"get_product_price\", tool_node)\n",
        "    builder.add_edge(\"get_product_details\", END)\n",
        "    builder.add_edge(\"get_product_price\", END)\n",
        "\n",
        "    builder.set_entry_point(\"tools\")\n",
        "    builder.add_conditional_edges(\"tools\", router)\n",
        "\n",
        "    app = builder.compile()\n",
        "    chat_history = langchain_load_dump.dumpd(app.invoke(HumanMessage(input)))\n",
        "    return parse_messages_to_output_dictionary(chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HGcs6PVjRj_"
      },
      "source": [
        "### Probar el agente\n",
        "\n",
        "Consulta tu agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGb58OJkjUs9"
      },
      "outputs": [],
      "source": [
        "response = agent_parsed_outcome(input=\"Get product details for shoes\")\n",
        "display(Markdown(format_output_as_markdown(response)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wCFstt8w4Dx"
      },
      "outputs": [],
      "source": [
        "response = agent_parsed_outcome(input=\"Get product price for shoes\")\n",
        "display(Markdown(format_output_as_markdown(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGPePsorpUl"
      },
      "source": [
        "## Evaluar un agente LangGraph con Vertex AI Gen AI Evaluation\n",
        "\n",
        "Cuando trabajas con agentes de IA, es importante realizar un seguimiento de su rendimiento y de qué tan bien están funcionando. Puedes ver esto de dos maneras principales: **monitoreo** y **observabilidad**.\n",
        "\n",
        "El monitoreo se enfoca en qué tan bien tu agente está realizando tareas específicas:\n",
        "\n",
        "* **Selección de una sola herramienta**: ¿El agente está eligiendo las herramientas adecuadas para el trabajo?\n",
        "\n",
        "* **Selección de múltiples herramientas (o trayectoria)**: ¿El agente está tomando decisiones lógicas en el orden en que usa las herramientas?\n",
        "\n",
        "* **Generación de respuestas**: ¿La salida del agente es buena y tiene sentido en función de las herramientas que utilizó?\n",
        "\n",
        "La observabilidad se trata de comprender la salud general del agente:\n",
        "\n",
        "* **Latencia**: ¿Cuánto tiempo tarda el agente en responder?\n",
        "\n",
        "* **Tasa de fallos**: ¿Con qué frecuencia el agente no produce una respuesta?\n",
        "\n",
        "El servicio Vertex AI Gen AI Evaluation te ayuda a evaluar todos estos aspectos tanto mientras estás prototipando el agente como después de desplegarlo en producción. Proporciona [criterios y métricas de evaluación preconstruidos](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) para que puedas ver exactamente cómo están funcionando tus agentes e identificar áreas de mejora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Preparar el conjunto de datos de evaluación del agente\n",
        "\n",
        "Para evaluar tu agente de IA utilizando el servicio Vertex AI Gen AI Evaluation, necesitas un conjunto de datos específico dependiendo de qué aspectos deseas evaluar de tu agente.\n",
        "\n",
        "Este conjunto de datos debe incluir los prompts dados al agente. También puede contener la respuesta ideal o esperada (ground truth) y la secuencia de llamadas a herramientas que el agente debería tomar (trayectoria de referencia) que representa la secuencia de herramientas que esperas que el agente llame para cada prompt dado.\n",
        "\n",
        "> Opcionalmente, puedes proporcionar tanto las respuestas generadas como la trayectoria predicha (**escenario de Trae-Tu-Propio-Conjunto-de-Datos**).\n",
        "\n",
        "A continuación tienes un ejemplo de conjunto de datos que podrías tener con un agente de soporte al cliente con el prompt del usuario y la trayectoria de referencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFf8uTdUiDt3"
      },
      "outputs": [],
      "source": [
        "eval_data = {\n",
        "    \"prompt\": [\n",
        "        \"Get price for smartphone\",\n",
        "        \"Get product details and price for headphones\",\n",
        "        \"Get details for usb charger\",\n",
        "        \"Get product details and price for shoes\",\n",
        "        \"Get product details for speaker?\",\n",
        "    ],\n",
        "    \"reference_trajectory\": [\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"smartphone\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"usb charger\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"shoes\"},\n",
        "            },\n",
        "            {\"tool_name\": \"get_product_price\", \"tool_input\": {\"product_name\": \"shoes\"}},\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"speaker\"},\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "}\n",
        "\n",
        "eval_sample_dataset = pd.DataFrame(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQEI1EcfvFHb"
      },
      "source": [
        "Imprimir algunas muestras del conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjsonqWWvIvE"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(eval_sample_dataset, num_rows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4CvBuf1afHG"
      },
      "source": [
        "### Evaluación del uso de una única herramienta\n",
        "\n",
        "Después de configurar tu agente de IA y el conjunto de datos de evaluación, comienzas evaluando si el agente está eligiendo la herramienta correcta para una tarea dada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rS5GGKHd5bx"
      },
      "source": [
        "#### Configurar métricas de uso de una única herramienta\n",
        "\n",
        "La métrica `trajectory_single_tool_use` en Vertex AI Gen AI Evaluation te da una forma rápida de evaluar si tu agente está usando la herramienta que esperas que use, sin importar el orden específico de las herramientas. Es una forma básica pero útil de comenzar a evaluar si se utilizó la herramienta correcta en algún momento durante el proceso del agente.\n",
        "\n",
        "Para usar la métrica `trajectory_single_tool_use`, necesitas configurar qué herramienta debería haberse usado para una solicitud particular del usuario. Por ejemplo, si un usuario pide \"enviar un correo electrónico\", podrías esperar que el agente use una herramienta \"send_email\", y especificarías el nombre de esa herramienta al usar esta métrica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xixvq8dwd5by"
      },
      "outputs": [],
      "source": [
        "single_tool_usage_metrics = [TrajectorySingleToolUse(tool_name=\"get_product_price\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKZoT2Qd5by"
      },
      "source": [
        "#### Ejecutar una tarea de evaluación\n",
        "\n",
        "Para ejecutar la evaluación, inicias una `EvalTask` utilizando el conjunto de datos predefinido (`eval_sample_dataset`) y las métricas (`single_tool_usage_metrics` en este caso) dentro de un experimento. Luego, ejecutas la evaluación utilizando la función agent_parsed_outcome y asignas un identificador único a esta ejecución de evaluación específica, almacenando y visualizando los resultados de la evaluación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRv43fDcd5by"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN = f\"single-metric-eval-{get_id()}\"\n",
        "\n",
        "single_tool_call_eval_task = EvalTask(\n",
        "    dataset=eval_sample_dataset,\n",
        "    metrics=single_tool_usage_metrics,\n",
        "    experiment=EXPERIMENT_NAME,\n",
        ")\n",
        "\n",
        "single_tool_call_eval_result = single_tool_call_eval_task.evaluate(\n",
        "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
        ")\n",
        "\n",
        "display_eval_report(single_tool_call_eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o5BjSTFKVMS"
      },
      "source": [
        "#### Visualizar resultados de la evaluación\n",
        "\n",
        "Usa algunas funciones auxiliares para visualizar una muestra del resultado de la evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jopzw83k14w"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(single_tool_call_eval_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlujdJpu5Kn6"
      },
      "source": [
        "### Evaluación de trayectoria\n",
        "\n",
        "Después de evaluar la capacidad del agente para seleccionar la herramienta única más adecuada para una tarea dada, generalizas la evaluación analizando las elecciones de secuencia de herramientas con respecto a la entrada del usuario (trayectoria). Esto evalúa si el agente no solo elige las herramientas correctas, sino que también las utiliza en un orden racional y efectivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s-nHdDJneHM"
      },
      "source": [
        "#### Configurar métricas de trayectoria\n",
        "\n",
        "Para evaluar la trayectoria del agente, Vertex AI Gen AI Evaluation proporciona varias métricas basadas en la verdad fundamental:\n",
        "\n",
        "* `trajectory_exact_match`: trayectorias idénticas (mismas acciones, mismo orden)\n",
        "\n",
        "* `trajectory_in_order_match`: acciones de referencia presentes en la trayectoria predicha, en orden (se permiten extras)\n",
        "\n",
        "* `trajectory_any_order_match`: todas las acciones de referencia presentes en la trayectoria predicha (el orden y los extras no importan).\n",
        "\n",
        "* `trajectory_precision`: proporción de acciones predichas presentes en la referencia\n",
        "\n",
        "* `trajectory_recall`: proporción de acciones de referencia presentes en la predicción.\n",
        "\n",
        "Todas las métricas puntúan 0 o 1, excepto `trajectory_precision` y `trajectory_recall` que varían de 0 a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c32WIS95neHN"
      },
      "outputs": [],
      "source": [
        "trajectory_metrics = [\n",
        "    \"trajectory_exact_match\",\n",
        "    \"trajectory_in_order_match\",\n",
        "    \"trajectory_any_order_match\",\n",
        "    \"trajectory_precision\",\n",
        "    \"trajectory_recall\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF3jhTH3neHN"
      },
      "source": [
        "#### Ejecutar una tarea de evaluación\n",
        "\n",
        "Enviar una evaluación ejecutando el método `evaluate` de la nueva `EvalTask`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOdS7TJUneHN"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN = f\"trajectory-{get_id()}\"\n",
        "\n",
        "trajectory_eval_task = EvalTask(\n",
        "    dataset=eval_sample_dataset, metrics=trajectory_metrics, experiment=EXPERIMENT_NAME\n",
        ")\n",
        "\n",
        "trajectory_eval_result = trajectory_eval_task.evaluate(\n",
        "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
        ")\n",
        "\n",
        "display_eval_report(trajectory_eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBiUI3LyLBtj"
      },
      "source": [
        "#### Visualizar resultados de la evaluación\n",
        "\n",
        "Imprimir y visualizar una muestra de los resultados de la evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLVRdN5llA0h"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(trajectory_eval_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erYYZEaaTNjJ"
      },
      "outputs": [],
      "source": [
        "plot_bar_plot(\n",
        "    trajectory_eval_result,\n",
        "    title=\"Métricas de Trayectoria\",\n",
        "    metrics=[f\"{metric}/mean\" for metric in trajectory_metrics],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8TipU2akHEd"
      },
      "source": [
        "### Evaluar la respuesta final\n",
        "\n",
        "Al igual que la evaluación de modelos, puedes evaluar la respuesta final del agente utilizando Vertex AI Gen AI Evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeK-py7ykkDN"
      },
      "source": [
        "#### Configurar métricas de respuesta\n",
        "\n",
        "Después de la inferencia del agente, Vertex AI Gen AI Evaluation proporciona varias métricas para evaluar las respuestas generadas. Puedes usar métricas basadas en cálculos para comparar la respuesta con una referencia (si es necesario) y usar métricas basadas en modelos existentes o personalizadas para determinar la calidad de la respuesta final.\n",
        "\n",
        "Consulta la [documentación](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) para obtener más información.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyGHGgeVklvz"
      },
      "outputs": [],
      "source": [
        "response_metrics = [\"safety\", \"coherence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaBJWcg1kn55"
      },
      "source": [
        "#### Ejecutar una tarea de evaluación\n",
        "\n",
        "Para evaluar las respuestas generadas por el agente, usa el método `evaluate` de la clase EvalTask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRb2EC_hknSD"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN = f\"response-{get_id()}\"\n",
        "\n",
        "response_eval_task = EvalTask(\n",
        "    dataset=eval_sample_dataset, metrics=response_metrics, experiment=EXPERIMENT_NAME\n",
        ")\n",
        "\n",
        "response_eval_result = response_eval_task.evaluate(\n",
        "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
        ")\n",
        "\n",
        "display_eval_report(response_eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOP9hW-rTUIU"
      },
      "source": [
        "#### Visualizar resultados de la evaluación\n",
        "\n",
        "\n",
        "Imprimir nueva muestra de resultados de la evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZODTRuq2lF75"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(response_eval_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntRBK3Te6PEc"
      },
      "source": [
        "### Evaluar la respuesta generada condicionada por la elección de herramientas\n",
        "\n",
        "Cuando evalúas agentes de IA que interactúan con entornos, las métricas estándar de generación de texto como la coherencia pueden no ser suficientes. Esto se debe a que estas métricas se enfocan principalmente en la estructura del texto, mientras que las respuestas del agente deben evaluarse en función de su efectividad dentro del entorno.\n",
        "\n",
        "En su lugar, usa métricas personalizadas que evalúan si la respuesta del agente sigue lógicamente de sus elecciones de herramientas como la que tienes en esta sección."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bENwFcd6prX"
      },
      "source": [
        "#### Definir una métrica personalizada\n",
        "\n",
        "Según la [documentación](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#model-based-metrics), puedes definir una plantilla de prompt para evaluar si la respuesta de un agente de IA sigue lógicamente de sus acciones configurando criterios y un sistema de calificación para esta evaluación.\n",
        "\n",
        "Define un `criteria` para establecer las pautas de evaluación y un `pointwise_rating_rubric` para proporcionar un sistema de puntuación (1 o 0). Luego usa un `PointwiseMetricPromptTemplate` para crear la plantilla utilizando estos componentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txGEHcg76riI"
      },
      "outputs": [],
      "source": [
        "criteria = {\n",
        "    \"Follows trajectory\": (\n",
        "        \"Evalúa si la respuesta del agente sigue lógicamente de la \"\n",
        "        \"secuencia de acciones que tomó. Considera estos subpuntos:\\n\"\n",
        "        \"  - ¿La respuesta refleja la información recopilada durante la trayectoria?\\n\"\n",
        "        \"  - ¿La respuesta es consistente con los objetivos y restricciones de la tarea?\\n\"\n",
        "        \"  - ¿Hay saltos inesperados o ilógicos en el razonamiento?\\n\"\n",
        "        \"Proporciona ejemplos específicos de la trayectoria y la respuesta para respaldar tu evaluación.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "pointwise_rating_rubric = {\n",
        "    \"1\": \"Sigue la trayectoria\",\n",
        "    \"0\": \"No sigue la trayectoria\",\n",
        "}\n",
        "\n",
        "response_follows_trajectory_prompt_template = PointwiseMetricPromptTemplate(\n",
        "    criteria=criteria,\n",
        "    rating_rubric=pointwise_rating_rubric,\n",
        "    input_variables=[\"prompt\", \"predicted_trajectory\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MJqXu0kikxd"
      },
      "source": [
        "Imprimir los datos del prompt de esta plantilla que contienen la información combinada de criterios y rúbrica lista para su uso en una evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EL7iEDMikNQ"
      },
      "outputs": [],
      "source": [
        "print(response_follows_trajectory_prompt_template.prompt_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1djVp7Fi4Yy"
      },
      "source": [
        "Después de definir la plantilla de prompt de evaluación, configura la métrica asociada para evaluar qué tan bien una respuesta sigue una trayectoria específica. El `PointwiseMetric` crea una métrica donde `response_follows_trajectory` es el nombre de la métrica y `response_follows_trajectory_prompt_template` proporciona instrucciones o contexto para la evaluación que configuraste antes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx1xbZD87iMj"
      },
      "outputs": [],
      "source": [
        "response_follows_trajectory_metric = PointwiseMetric(\n",
        "    metric=\"response_follows_trajectory\",\n",
        "    metric_prompt_template=response_follows_trajectory_prompt_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pmxLwTe7Ywv"
      },
      "source": [
        "#### Configurar métricas de respuesta\n",
        "\n",
        "Configurar nuevas métricas de evaluación de respuestas generadas incluyendo la métrica personalizada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrsbVFDd7Ywv"
      },
      "outputs": [],
      "source": [
        "response_tool_metrics = [\n",
        "    \"trajectory_exact_match\",\n",
        "    \"trajectory_in_order_match\",\n",
        "    \"safety\",\n",
        "    response_follows_trajectory_metric,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo-Sza807Ywv"
      },
      "source": [
        "#### Ejecutar una tarea de evaluación\n",
        "\n",
        "Ejecutar una nueva evaluación del agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dkb4gSn7Ywv"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN = f\"response-over-tools-{get_id()}\"\n",
        "\n",
        "response_eval_tool_task = EvalTask(\n",
        "    dataset=eval_sample_dataset,\n",
        "    metrics=response_tool_metrics,\n",
        "    experiment=EXPERIMENT_NAME,\n",
        ")\n",
        "\n",
        "response_eval_tool_result = response_eval_tool_task.evaluate(\n",
        "    runnable=agent_parsed_outcome, experiment_run_name=EXPERIMENT_RUN\n",
        ")\n",
        "\n",
        "display_eval_report(response_eval_tool_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtOfIFi2j88g"
      },
      "source": [
        "#### Visualizar resultados de la evaluación\n",
        "\n",
        "Visualizar muestra de resultados de la evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH2YvXgLlLH7"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(response_eval_tool_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nuUDP3a2eTB"
      },
      "source": [
        "## Bonus: Trae-Tu-Propio-Conjunto-de-Datos (BYOD) y evalúa un agente LangGraph utilizando Vertex AI Gen AI Evaluation\n",
        "\n",
        "En escenarios de Trae-Tu-Propio-Conjunto-de-Datos (BYOD) [escenarios](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-dataset), proporcionas tanto la trayectoria predicha como la respuesta generada por el agente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRLKlmWd27PK"
      },
      "source": [
        "### Trae tu propio conjunto de datos de evaluación\n",
        "\n",
        "Define el conjunto de datos de evaluación con la trayectoria predicha y la respuesta generada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9hBgsg324Ej"
      },
      "outputs": [],
      "source": [
        "byod_eval_data = {\n",
        "    \"prompt\": [\n",
        "        \"Get price for smartphone\",\n",
        "        \"Get product details and price for headphones\",\n",
        "        \"Get details for usb charger\",\n",
        "        \"Get product details and price for shoes\",\n",
        "        \"Get product details for speaker?\",\n",
        "    ],\n",
        "    \"reference_trajectory\": [\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"smartphone\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"usb charger\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"shoes\"},\n",
        "            },\n",
        "            {\"tool_name\": \"get_product_price\", \"tool_input\": {\"product_name\": \"shoes\"}},\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"speaker\"},\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    \"predicted_trajectory\": [\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"smartphone\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"usb charger\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"shoes\"},\n",
        "            },\n",
        "            {\"tool_name\": \"get_product_price\", \"tool_input\": {\"product_name\": \"shoes\"}},\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"speaker\"},\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "    \"response\": [\n",
        "        500,\n",
        "        50,\n",
        "        \"A super fast and light usb charger\",\n",
        "        100,\n",
        "        \"A voice-controlled smart speaker that plays music, sets alarms, and controls smart home devices.\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "byod_eval_sample_dataset = pd.DataFrame(byod_eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEYmU2eJ7q-1"
      },
      "source": [
        "### Ejecutar una tarea de evaluación\n",
        "\n",
        "Ejecutar una nueva evaluación del agente utilizando tu propio conjunto de datos y la misma configuración de la última evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBD-4wpB7q-3"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN_NAME = f\"response-over-tools-byod-{get_id()}\"\n",
        "\n",
        "byod_response_eval_tool_task = EvalTask(\n",
        "    dataset=byod_eval_sample_dataset,\n",
        "    metrics=response_tool_metrics,\n",
        "    experiment=EXPERIMENT_NAME,\n",
        ")\n",
        "\n",
        "byod_response_eval_tool_result = byod_response_eval_tool_task.evaluate(\n",
        "    experiment_run_name=EXPERIMENT_RUN_NAME\n",
        ")\n",
        "\n",
        "display_eval_report(byod_response_eval_tool_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eU3LG6r7q-3"
      },
      "source": [
        "#### Visualizar resultados de la evaluación\n",
        "\n",
        "Visualizar muestra de resultados de la evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQFzmd2I7q-3"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(byod_response_eval_tool_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJr8GqQKTpUa"
      },
      "outputs": [],
      "source": [
        "display_radar_plot(\n",
        "    byod_response_eval_tool_result,\n",
        "    title=\"Métricas de evaluación del agente\",\n",
        "    metrics=[f\"{metric}/mean\" for metric in response_tool_metrics],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## Limpiar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox2I3UfRlTOd"
      },
      "outputs": [],
      "source": [
        "delete_experiment = True\n",
        "\n",
        "if delete_experiment:\n",
        "    try:\n",
        "        experiment = aiplatform.Experiment(EXPERIMENT_NAME)\n",
        "        experiment.delete(delete_backing_tensorboard_runs=True)\n",
        "    except Exception as e:\n",
        "        print(e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "evaluating_langgraph_agent.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
